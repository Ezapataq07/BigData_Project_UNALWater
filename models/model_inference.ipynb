{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657e5fb9-24d4-4ced-ad98-c84edec58b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from pyspark.sql.functions import max, col, lit, expr\n",
    "import datetime as dt\n",
    "import mlflow\n",
    "fe = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddeb2c10-2da6-4001-962f-0ec6bb0ef8b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caadc058-f434-43ba-bad0-991f68a0c6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04b3515-d87a-4458-8453-bdd8e1dccddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "today = dt.date.today()\n",
    "initial_date = today - dt.timedelta(days=n)\n",
    "df_dataset = fe.read_table(name=f'{catalog_name}.{gold_schema_name}.features_demand_forecast').filter(col('event_date') >= initial_date)\n",
    "model = mlflow.spark.load_model(f'models:/{catalog_name}.{gold_schema_name}.demand_forecast_model@prod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54389a18-e188-4806-9285-243a16ff7b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessing data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1782171a-6071-4255-bca8-7cf93d12798f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_to_predict = (\n",
    "    df_dataset\n",
    "    .groupBy('district').agg(max('event_datetime').alias('event_datetime'))\n",
    "    .join(df_dataset, on=['district','event_datetime'], how='left')\n",
    "    .withColumn('event_datetime', expr(\"event_datetime + INTERVAL 1 HOUR\"))\n",
    "    .withColumn('event_date', expr(\"date(event_datetime)\"))\n",
    "    .withColumn('event_hour', expr(\"hour(event_datetime)\"))\n",
    "    .withColumn('event_weekday', expr(\"dayofweek(event_datetime)\"))\n",
    "    .withColumn('prev_quantity_products_6', col('prev_quantity_products_5'))\n",
    "    .withColumn('prev_quantity_products_5', col('prev_quantity_products_4'))\n",
    "    .withColumn('prev_quantity_products_4', col('prev_quantity_products_3'))\n",
    "    .withColumn('prev_quantity_products_3', col('prev_quantity_products_2'))\n",
    "    .withColumn('prev_quantity_products_2', col('prev_quantity_products'))\n",
    "    .withColumn('prev_quantity_products', col('sum_quantity_products'))\n",
    "    .drop('sum_quantity_products')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe7ef75-945d-4b47-afe2-bdc2c08117ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b55ad23-1e2a-47ca-bb91-5e4dfbd42ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(df_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc38805c-4523-4a3f-a3e2-2d1c8e7c596a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    predictions\n",
    "    .withColumn('sum_quantity_products', col('prediction'))\n",
    "    .drop(*['district_index','district_vec','event_weekday_vec','event_hour_vec','features1','features2','features3','features4','features5','features6','prediction'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b760bc18-ffaa-4c26-bfbf-63633595c9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = df_dataset.unionByName(predictions).select('district', 'event_datetime', 'sum_quantity_products')\n",
    "final_df.createOrReplaceTempView('final_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c370d81-6c49-45d2-a472-31a0af660e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615dacbc-b758-426a-bcc1-a01d34072f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {catalog_name}.{gold_schema_name}.demand_forecast AS target\n",
    "        USING final_df AS source\n",
    "        ON target.district = source.district AND target.event_datetime = source.event_datetime\n",
    "        WHEN MATCHED THEN\n",
    "        UPDATE SET target.sum_quantity_products = source.sum_quantity_products\n",
    "        WHEN NOT MATCHED THEN\n",
    "        INSERT (district, event_datetime, sum_quantity_products)\n",
    "        VALUES (source.district, source.event_datetime, source.sum_quantity_products)\n",
    "          \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
